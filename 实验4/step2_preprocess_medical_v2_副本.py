#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›ç‰ˆé¢„å¤„ç†è„šæœ¬ - æ›´ç»†ç²’åº¦çš„åˆ†å—
"""

import os
import json
import re

def load_medical_data(file_path):
    """åŠ è½½medical.jsonæ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    context_text = data.get('context', '')
    corpus_name = data.get('corpus_name', 'medical')
    
    print(f"ğŸ“Š æ•°æ®é›†åç§°: {corpus_name}")
    print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(context_text)} å­—ç¬¦")
    
    return context_text, corpus_name

def split_text_intelligently(text, max_chunk_size=1000, overlap=100):
    """
    æ™ºèƒ½åˆ†å—ï¼šè¯†åˆ«è‡ªç„¶æ ‡é¢˜å¹¶åˆ†å‰²
    
    è§‚å¯Ÿåˆ°çš„æ¨¡å¼ï¼š
    1. æ•°å­—+ç©ºæ ¼+å¤§å†™æ ‡é¢˜ï¼ˆå¦‚"7 Adrenal glands"ï¼‰
    2. å¯èƒ½è¿˜æœ‰å…¶ä»–æ ‡é¢˜æ¨¡å¼
    """
    print("ğŸ”ª å¼€å§‹æ™ºèƒ½åˆ†å—...")
    
    # æ¨¡å¼1ï¼šæ•°å­—å¼€å¤´+ç©ºæ ¼+å¤§å†™å•è¯ï¼ˆå¯èƒ½çš„ç« èŠ‚æ ‡é¢˜ï¼‰
    # ä¾‹å¦‚: "7 Adrenal glands", "8 Adrenal tumors"
    title_pattern1 = r'\n(\d+\s+[A-Z][a-z]+(?:\s+[A-Za-z]+)*)'
    
    # æ¨¡å¼2ï¼šçº¯å¤§å†™å•è¯ï¼ˆå¯èƒ½çš„æ ‡é¢˜ï¼‰
    # ä¾‹å¦‚: "KEY POINTS", "SIGNS AND SYMPTOMS"
    title_pattern2 = r'\n([A-Z][A-Z\s]+[A-Z])'
    
    # æ¨¡å¼3ï¼šå¸¸è§åŒ»ç–—ç« èŠ‚æ ‡é¢˜
    medical_sections = [
        r'\n(About\s+.+)',
        r'\n(What is\s+.+\?)',
        r'\n(How is\s+.+\?)',
        r'\n(Signs and symptoms)',
        r'\n(Risk factors)',
        r'\n(Diagnosis)',
        r'\n(Treatment)',
        r'\n(Key points)',
    ]
    
    # æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„æ ‡é¢˜ä½ç½®
    titles = []
    
    # æŸ¥æ‰¾æ¨¡å¼1
    for match in re.finditer(title_pattern1, text):
        titles.append((match.start(), match.group(1), "æ•°å­—æ ‡é¢˜"))
    
    # æŸ¥æ‰¾æ¨¡å¼2
    for match in re.finditer(title_pattern2, text):
        # è¿‡æ»¤æ‰å¤ªçŸ­çš„ï¼ˆå¯èƒ½ä¸æ˜¯æ ‡é¢˜ï¼‰
        if len(match.group(1).strip()) > 5:
            titles.append((match.start(), match.group(1).strip(), "å¤§å†™æ ‡é¢˜"))
    
    # æŸ¥æ‰¾åŒ»ç–—ç« èŠ‚
    for pattern in medical_sections:
        for match in re.finditer(pattern, text, re.IGNORECASE):
            titles.append((match.start(), match.group(1), "åŒ»ç–—ç« èŠ‚"))
    
    # æŒ‰ä½ç½®æ’åºå¹¶å»é‡ï¼ˆç›¸è¿‘ä½ç½®åªä¿ç•™ä¸€ä¸ªï¼‰
    titles.sort(key=lambda x: x[0])
    
    # åˆå¹¶ç›¸è¿‘çš„æ ‡é¢˜ï¼ˆ50å­—ç¬¦å†…ï¼‰
    unique_titles = []
    for title in titles:
        if not unique_titles or title[0] - unique_titles[-1][0] > 50:
            unique_titles.append(title)
    
    print(f"  å‘ç° {len(unique_titles)} ä¸ªæ½œåœ¨æ ‡é¢˜åˆ†å‰²ç‚¹")
    
    # æ˜¾ç¤ºå‰10ä¸ªæ ‡é¢˜
    print("  å‰10ä¸ªæ ‡é¢˜:")
    for i, (pos, title_text, title_type) in enumerate(unique_titles[:10]):
        print(f"    {i+1}. ä½ç½®{pos}: [{title_type}] {title_text}")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œå›é€€åˆ°å›ºå®šé•¿åº¦åˆ†å—
    if len(unique_titles) < 5:
        print("  è­¦å‘Šï¼šæ ‡é¢˜ç‚¹å¤ªå°‘ï¼Œä½¿ç”¨å›ºå®šé•¿åº¦åˆ†å—")
        return split_text_fixed_length(text, max_chunk_size, overlap)
    
    # æ ¹æ®æ ‡é¢˜åˆ†å‰²æ–‡æœ¬
    chunks = []
    last_pos = 0
    
    for i in range(len(unique_titles)):
        pos, title_text, title_type = unique_titles[i]
        
        # å½“å‰æ ‡é¢˜åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ï¼ˆæˆ–ç»“å°¾ï¼‰ä¹‹é—´çš„æ–‡æœ¬
        if i + 1 < len(unique_titles):
            next_pos = unique_titles[i + 1][0]
            chunk_text = text[last_pos:next_pos].strip()
        else:
            chunk_text = text[last_pos:].strip()
        
        # å¦‚æœchunkå¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
        if chunk_text and len(chunk_text) > 0:
            if len(chunk_text) > max_chunk_size * 1.5:
                # å¯¹é•¿chunkè¿›è¡ŒäºŒæ¬¡åˆ†å‰²
                sub_chunks = split_text_fixed_length(chunk_text, max_chunk_size, overlap)
                chunks.extend(sub_chunks)
            else:
                chunks.append(chunk_text)
        
        last_pos = pos
    
    # æ·»åŠ æœ€åä¸€éƒ¨åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
    if last_pos < len(text):
        final_chunk = text[last_pos:].strip()
        if final_chunk:
            if len(final_chunk) > max_chunk_size * 1.5:
                sub_chunks = split_text_fixed_length(final_chunk, max_chunk_size, overlap)
                chunks.extend(sub_chunks)
            else:
                chunks.append(final_chunk)
    
    return chunks

def split_text_fixed_length(text, chunk_size=1000, overlap=100):
    """å›ºå®šé•¿åº¦åˆ†å—ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œå¤„ç»“æŸ
        if end < len(text):
            # å¯»æ‰¾æœ€è¿‘çš„å¥å­ç»“æŸç¬¦
            for punct in ['ã€‚', '.', '!', '?', '\n']:
                punct_pos = text.rfind(punct, end - 50, end + 50)
                if punct_pos != -1 and punct_pos > start:
                    end = punct_pos + 1
                    break
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        start = end - overlap  # è®¾ç½®é‡å 
        if start <= 0:
            break
    
    return chunks

def save_chunks_to_json(chunks, corpus_name, output_path):
    """ä¿å­˜chunksä¸ºMilvuså¯ç”¨çš„JSONæ ¼å¼"""
    
    milvus_data = []
    
    for i, chunk in enumerate(chunks):
        # æå–chunkçš„å‰50å­—ç¬¦ä½œä¸ºæ ‡é¢˜
        preview = chunk[:50].replace('\n', ' ')
        
        entry = {
            "id": f"{corpus_name}_{i:06d}",
            "title": f"{preview}...",
            "abstract": chunk,
            "source_file": "medical.json",
            "chunk_index": i,
            "corpus_name": corpus_name,
            "chunk_length": len(chunk)
        }
        milvus_data.append(entry)
    
    # ä¿å­˜ä¸ºJSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(milvus_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ æ•°æ®ä¿å­˜åˆ°: {output_path}")
    print(f"ğŸ“‹ æ€»è®°å½•æ•°: {len(milvus_data)}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    if milvus_data:
        lengths = [item["chunk_length"] for item in milvus_data]
        print(f"ğŸ“Š Chunké•¿åº¦ç»Ÿè®¡:")
        print(f"    å¹³å‡: {sum(lengths)/len(lengths):.0f} å­—ç¬¦")
        print(f"    æœ€å°: {min(lengths)} å­—ç¬¦")
        print(f"    æœ€å¤§: {max(lengths)} å­—ç¬¦")
        print(f"    <500å­—ç¬¦: {sum(1 for l in lengths if l < 500)} ä¸ª")
        print(f"    500-1500å­—ç¬¦: {sum(1 for l in lengths if 500 <= l < 1500)} ä¸ª")
        print(f"    >1500å­—ç¬¦: {sum(1 for l in lengths if l >= 1500)} ä¸ª")
    
    return milvus_data

def main():
    # é…ç½®å‚æ•°
    input_file = "./data/medical.json"
    output_file = "./data/processed_medical_v2.json"
    
    # åˆ†å—å‚æ•°
    MAX_CHUNK_SIZE = 1200  # æœ€å¤§å—å¤§å°ï¼ˆå­—ç¬¦ï¼‰
    OVERLAP = 150          # é‡å å¤§å°
    
    print("=" * 60)
    print("åŒ»ç–—æ•°æ®é¢„å¤„ç†è„šæœ¬ V2ï¼ˆæ™ºèƒ½åˆ†å—ï¼‰")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    text, corpus_name = load_medical_data(input_file)
    
    # 2. æ™ºèƒ½åˆ†å—
    chunks = split_text_intelligently(
        text, 
        max_chunk_size=MAX_CHUNK_SIZE,
        overlap=OVERLAP
    )
    
    print(f"\nâœ… ç”Ÿæˆ {len(chunks)} ä¸ªchunks")
    
    # 3. ä¿å­˜å¤„ç†ç»“æœ
    save_chunks_to_json(chunks, corpus_name, output_file)
    
    # 4. æ˜¾ç¤ºæ ·ä¾‹
    print(f"\nğŸ” å¤„ç†ç»“æœæ ·ä¾‹ (å‰3ä¸ªchunk):")
    for i in range(min(3, len(chunks))):
        print(f"\nChunk {i} (é•¿åº¦: {len(chunks[i])} å­—ç¬¦):")
        print("-" * 50)
        print(chunks[i][:200] + "..." if len(chunks[i]) > 200 else chunks[i])
        print("-" * 50)

if __name__ == "__main__":
    main()