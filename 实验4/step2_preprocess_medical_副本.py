#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¢„å¤„ç† medical.json æ•°æ®
å°†é•¿æ–‡æœ¬åˆ†å‰²æˆchunkså¹¶ä¿å­˜ä¸ºprocessed_data.json
"""

import os
import json
import re

def load_medical_data(file_path):
    """åŠ è½½medical.jsonæ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå–æ–‡æœ¬å†…å®¹
    context_text = data.get('context', '')
    corpus_name = data.get('corpus_name', 'medical')
    
    print(f"ğŸ“Š æ•°æ®é›†åç§°: {corpus_name}")
    print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(context_text)} å­—ç¬¦")
    
    if not context_text:
        print("âŒ é”™è¯¯: contextå­—æ®µä¸ºç©º")
        return None
    
    return context_text, corpus_name

def split_text_by_paragraphs(text, max_chunk_size=1000, min_chunk_size=200):
    """
    æ™ºèƒ½åˆ†å—ç­–ç•¥ï¼šå…ˆæŒ‰æ®µè½åˆ†ï¼Œå†å¯¹é•¿æ®µè½è¿›è¡ŒäºŒæ¬¡åˆ†å‰²
    
    Args:
        text: é•¿æ–‡æœ¬
        max_chunk_size: æœ€å¤§å—å¤§å°
        min_chunk_size: æœ€å°å—å¤§å°
    """
    print("ğŸ”ª å¼€å§‹æ–‡æœ¬åˆ†å—...")
    
    # 1. å…ˆæŒ‰æ¢è¡Œç¬¦åˆ†å‰²æˆæ®µè½
    raw_paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
    print(f"  åŸå§‹æ®µè½æ•°: {len(raw_paragraphs)}")
    
    chunks = []
    
    # 2. å¤„ç†æ¯ä¸ªæ®µè½
    for i, paragraph in enumerate(raw_paragraphs):
        if len(paragraph) <= max_chunk_size:
            # æ®µè½é•¿åº¦åˆé€‚ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªchunk
            if len(paragraph) >= min_chunk_size:
                chunks.append(paragraph)
            else:
                # å¤ªçŸ­çš„æ®µè½ï¼Œå°è¯•ä¸ä¸‹ä¸€ä¸ªæ®µè½åˆå¹¶
                if i + 1 < len(raw_paragraphs):
                    combined = paragraph + " " + raw_paragraphs[i + 1]
                    if len(combined) <= max_chunk_size:
                        chunks.append(combined)
                    else:
                        # åˆå¹¶åè¿˜æ˜¯å¤ªé•¿ï¼Œå„è‡ªå¤„ç†
                        if len(paragraph) >= min_chunk_size:
                            chunks.append(paragraph)
                else:
                    # æœ€åä¸€ä¸ªæ®µè½ï¼Œå¦‚æœå¤ªçŸ­ä½†æœ‰ä¸€å®šé•¿åº¦ï¼Œè¿˜æ˜¯ä¿ç•™
                    if len(paragraph) >= 50:  # è‡³å°‘50å­—ç¬¦
                        chunks.append(paragraph)
        else:
            # æ®µè½å¤ªé•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            # æŒ‰å¥å­åˆ†å‰²ï¼ˆç®€å•æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²ï¼‰
            sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', paragraph)
            sentences = [s.strip() for s in sentences if s.strip()]
            
            current_chunk = ""
            for sentence in sentences:
                # å¦‚æœå½“å‰chunkåŠ ä¸Šæ–°å¥å­ä¸è¶…è¿‡max_sizeï¼Œå°±åˆå¹¶
                if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
                    current_chunk = current_chunk + sentence if not current_chunk else current_chunk + " " + sentence
                else:
                    # ä¿å­˜å½“å‰chunkï¼Œå¼€å§‹æ–°çš„chunk
                    if current_chunk and len(current_chunk) >= min_chunk_size:
                        chunks.append(current_chunk)
                    current_chunk = sentence
            
            # æ·»åŠ æœ€åä¸€ä¸ªchunk
            if current_chunk and len(current_chunk) >= min_chunk_size:
                chunks.append(current_chunk)
    
    print(f"  ç”Ÿæˆchunkæ•°é‡: {len(chunks)}")
    
    # 3. ç»Ÿè®¡ä¿¡æ¯
    if chunks:
        avg_len = sum(len(c) for c in chunks) / len(chunks)
        min_len = min(len(c) for c in chunks)
        max_len = max(len(c) for c in chunks)
        
        print(f"  Chunké•¿åº¦ç»Ÿè®¡:")
        print(f"    å¹³å‡: {avg_len:.0f} å­—ç¬¦")
        print(f"    æœ€å°: {min_len} å­—ç¬¦")
        print(f"    æœ€å¤§: {max_len} å­—ç¬¦")
    
    return chunks

def save_chunks_to_json(chunks, corpus_name, output_path):
    """ä¿å­˜chunksä¸ºMilvuså¯ç”¨çš„JSONæ ¼å¼"""
    
    milvus_data = []
    
    for i, chunk in enumerate(chunks):
        entry = {
            "id": f"{corpus_name}_{i:06d}",
            "title": f"{corpus_name}_chunk_{i}",
            "abstract": chunk,
            "source_file": "medical.json",
            "chunk_index": i,
            "corpus_name": corpus_name
        }
        milvus_data.append(entry)
    
    # ä¿å­˜ä¸ºJSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(milvus_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ æ•°æ®ä¿å­˜åˆ°: {output_path}")
    print(f"ğŸ“‹ æ€»è®°å½•æ•°: {len(milvus_data)}")
    
    return milvus_data

def main():
    # é…ç½®å‚æ•°
    input_file = "./data/medical.json"
    output_file = "./data/processed_medical.json"
    
    # åˆ†å—å‚æ•°
    MAX_CHUNK_SIZE = 800  # æœ€å¤§å—å¤§å°ï¼ˆå­—ç¬¦ï¼‰
    MIN_CHUNK_SIZE = 100   # æœ€å°å—å¤§å°ï¼ˆå­—ç¬¦ï¼‰
    
    print("=" * 60)
    print("åŒ»ç–—æ•°æ®é¢„å¤„ç†è„šæœ¬")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    result = load_medical_data(input_file)
    if not result:
        return
    
    text, corpus_name = result
    
    # 2. åˆ†å—å¤„ç†
    chunks = split_text_by_paragraphs(
        text, 
        max_chunk_size=MAX_CHUNK_SIZE,
        min_chunk_size=MIN_CHUNK_SIZE
    )
    
    if not chunks:
        print("âŒ é”™è¯¯: æœªç”Ÿæˆä»»ä½•chunk")
        return
    
    # 3. ä¿å­˜å¤„ç†ç»“æœ
    save_chunks_to_json(chunks, corpus_name, output_file)
    
    # 4. æ˜¾ç¤ºæ ·ä¾‹
    print(f"\nğŸ” å¤„ç†ç»“æœæ ·ä¾‹ (å‰3ä¸ªchunk):")
    for i in range(min(3, len(chunks))):
        print(f"\nChunk {i}:")
        print("-" * 40)
        print(chunks[i][:200] + "..." if len(chunks[i]) > 200 else chunks[i])
        print("-" * 40)

if __name__ == "__main__":
    main()