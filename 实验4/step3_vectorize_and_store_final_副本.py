#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒ»ç–—RAGç³»ç»Ÿ - å‘é‡åŒ–ä¸å­˜å‚¨è„šæœ¬
é€‚é…ä¿®æ”¹åçš„ä¸­æ–‡é…ç½®
"""

# ========== è·¯å¾„ä¿®å¤ ==========
import os
import sys

# è·å–é¡¹ç›®æ ¹ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)  # ä¸Šä¸€çº§å°±æ˜¯é¡¹ç›®æ ¹ç›®å½•

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, project_root)

print(f"ğŸ“ è„šæœ¬ç›®å½•: {current_dir}")
print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print()
# ========== ç»“æŸè·¯å¾„ä¿®å¤ ==========

import json
import time

# ç°åœ¨åº”è¯¥å¯ä»¥æ­£å¸¸å¯¼å…¥äº†
from models_å‰¯æœ¬ import load_embedding_model
from milvus_utils import get_milvus_client, setup_milvus_collection
from config import (
    COLLECTION_NAME, EMBEDDING_DIM, EMBEDDING_MODEL_NAME, 
    DATA_FILE, id_to_doc_map
)

def load_and_prepare_data():
    """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {DATA_FILE}")
    
    if not os.path.exists(DATA_FILE):
        print(f"âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {DATA_FILE}")
        print("è¯·ç¡®ä¿ config.py ä¸­çš„ DATA_FILE è·¯å¾„æ­£ç¡®")
        return None, None
    
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“Š åŠ è½½ {len(data)} æ¡è®°å½•")
    
    # å‡†å¤‡æ–‡æœ¬å’Œå…ƒæ•°æ®
    texts = []
    metadata_list = []
    
    for i, item in enumerate(data):
        # æå–æ–‡æœ¬ï¼ˆå°è¯•ä¸åŒå­—æ®µåï¼‰
        text = item.get('abstract', item.get('text', item.get('content', '')))
        if not text or len(text.strip()) < 10:  # è·³è¿‡å¤ªçŸ­çš„æ–‡æœ¬
            continue
            
        texts.append(text)
        
        # æ„å»ºå…ƒæ•°æ®
        metadata = {
            'id': item.get('id', f"doc_{i}"),
            'title': item.get('title', f"Chunk {i}"),
            'content': text,  # ç”¨äºrag_core.pyæ£€ç´¢
            'abstract': text,
            'chunk_index': item.get('chunk_index', i),
            'source_file': item.get('source_file', 'medical.json')
        }
        metadata_list.append(metadata)
        
        # å¡«å……å…¨å±€æ˜ å°„ï¼ˆç”¨äºrag_core.pyï¼‰
        id_to_doc_map[i] = metadata
    
    print(f"âœ… å‡†å¤‡ {len(texts)} ä¸ªæœ‰æ•ˆæ–‡æœ¬")
    return texts, metadata_list

def batch_vectorize(texts, model):
    """åˆ†æ‰¹å‘é‡åŒ–æ–‡æœ¬"""
    print(f"ğŸ”¢ å¼€å§‹å‘é‡åŒ– {len(texts)} ä¸ªæ–‡æœ¬...")
    
    batch_size = 64
    all_embeddings = []
    
    total_batches = (len(texts) + batch_size - 1) // batch_size
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(texts))
        batch_texts = texts[start_idx:end_idx]
        
        # å‘é‡åŒ–
        batch_embeddings = model.encode(
            batch_texts, 
            normalize_embeddings=True,
            show_progress_bar=False
        )
        all_embeddings.extend(batch_embeddings)
        
        # æ˜¾ç¤ºè¿›åº¦
        progress = (batch_idx + 1) / total_batches * 100
        print(f"  è¿›åº¦: {end_idx}/{len(texts)} ({progress:.1f}%)")
        
        # æ¯10æ‰¹æˆ–æœ€åä¸€æ‰¹ä¿å­˜ä¸­é—´ç»“æœ
        if (batch_idx + 1) % 10 == 0 or batch_idx + 1 == total_batches:
            # å¯é€‰ï¼šä¿å­˜æ£€æŸ¥ç‚¹
            pass
    
    print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œç”Ÿæˆ {len(all_embeddings)} ä¸ªå‘é‡")
    if all_embeddings:
        print(f"ğŸ“ å‘é‡ç»´åº¦: {len(all_embeddings[0])} (åº”ä¸EMBEDDING_DIM={EMBEDDING_DIM}åŒ¹é…)")
    
    return all_embeddings

def store_in_milvus(embeddings, metadata_list):
    """å­˜å‚¨åˆ°Milvus"""
    print(f"ğŸ—„ï¸  è¿æ¥åˆ°Milvus...")
    
    # å°è¯•ç›´æ¥åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆé¿å…streamlitç¼“å­˜é—®é¢˜ï¼‰
    try:
        from pymilvus import MilvusClient
        client = MilvusClient("./milvus_lite_data.db")
        print("âœ… Milvuså®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆ›å»ºMilvuså®¢æˆ·ç«¯å¤±è´¥: {e}")
        print("å°è¯•ä½¿ç”¨get_milvus_client()...")
        client = get_milvus_client()
        if not client:
            return False
    
    # æ£€æŸ¥æˆ–åˆ›å»ºé›†åˆ
    print(f"ğŸ“‹ æ£€æŸ¥é›†åˆ: {COLLECTION_NAME}")
    
    # è·å–æ‰€æœ‰é›†åˆ
    collections = client.list_collections()
    print(f"  ç°æœ‰é›†åˆ: {collections}")
    
    if COLLECTION_NAME in collections:
        print(f"âš ï¸  é›†åˆå·²å­˜åœ¨ï¼Œæ­£åœ¨åˆ é™¤...")
        try:
            client.drop_collection(COLLECTION_NAME)
            print("âœ… æ—§é›†åˆå·²åˆ é™¤")
        except Exception as e:
            print(f"âŒ åˆ é™¤é›†åˆå¤±è´¥: {e}")
            # ç»§ç»­å°è¯•
    
    # è®¾ç½®æ–°é›†åˆ
    print(f"ğŸ”¨ åˆ›å»ºæ–°é›†åˆ...")
    setup_success = setup_milvus_collection(client)
    if not setup_success:
        print("å°è¯•æ‰‹åŠ¨åˆ›å»ºé›†åˆ...")
        # æ‰‹åŠ¨åˆ›å»ºé›†åˆ
        try:
            schema = client.create_schema(
                auto_id=False,
                enable_dynamic_field=True
            )
            schema.add_field(field_name="id", datatype=DataType.INT64, is_primary=True)
            schema.add_field(field_name="vector", datatype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM)
            schema.add_field(field_name="text", datatype=DataType.VARCHAR, max_length=65535)
            schema.add_field(field_name="title", datatype=DataType.VARCHAR, max_length=255)
            schema.add_field(field_name="doc_id", datatype=DataType.VARCHAR, max_length=255)
            schema.add_field(field_name="chunk_idx", datatype=DataType.INT64)
            
            client.create_collection(
                collection_name=COLLECTION_NAME,
                schema=schema
            )
            print("âœ… æ‰‹åŠ¨åˆ›å»ºé›†åˆæˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ‰‹åŠ¨åˆ›å»ºé›†åˆå¤±è´¥: {e}")
            return False
    
    # å‡†å¤‡æ’å…¥æ•°æ®
    print(f"ğŸ“¥ å‡†å¤‡æ’å…¥æ•°æ®...")
    
    insert_data = []
    for i, (embedding, metadata) in enumerate(zip(embeddings, metadata_list)):
        insert_data.append({
            "id": i,  # Milvuséœ€è¦æ•´æ•°ID
            "vector": embedding.tolist(),
            "text": metadata['content'],
            "title": metadata['title'],
            "doc_id": metadata['id'],
            "chunk_idx": metadata['chunk_index']
        })
    
    # åˆ†æ‰¹æ’å…¥
    batch_size = 100
    inserted_count = 0
    
    for i in range(0, len(insert_data), batch_size):
        batch = insert_data[i:i+batch_size]
        
        try:
            res = client.insert(collection_name=COLLECTION_NAME, data=batch)
            inserted_count += len(batch)
            print(f"  æ’å…¥æ‰¹æ¬¡ {i//batch_size + 1}: "
                  f"{inserted_count}/{len(insert_data)} æ¡")
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡æ’å…¥å¤±è´¥ï¼Œå°è¯•å•æ¡æ’å…¥: {e}")
            # å•æ¡æ’å…¥
            for item in batch:
                try:
                    client.insert(collection_name=COLLECTION_NAME, data=[item])
                    inserted_count += 1
                except Exception as e2:
                    print(f"   è·³è¿‡è®°å½•: {e2}")
    
    # åˆ›å»ºç´¢å¼•
    print(f"ğŸ” åˆ›å»ºå‘é‡ç´¢å¼•...")
    try:
        index_params = {
            "metric_type": "L2",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 256}
        }
        client.create_index(
            collection_name=COLLECTION_NAME,
            field_name="vector",
            index_params=index_params
        )
        print("âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸  ç´¢å¼•åˆ›å»ºå¤±è´¥ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    try:
        stats = client.get_collection_stats(collection_name=COLLECTION_NAME)
        print(f"ğŸ“Š é›†åˆç»Ÿè®¡:")
        print(f"  è®°å½•æ•°: {stats['row_count']}")
        # æ‰“å°å‰å‡ ä¸ªåˆ†åŒºä¿¡æ¯
        for i, partition in enumerate(stats['partitions'][:3]):
            print(f"  åˆ†åŒº{i}: {partition['segment_count']} segments")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•è·å–ç»Ÿè®¡: {e}")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("åŒ»ç–—RAGç³»ç»Ÿ - å‘é‡åŒ–ä¸å­˜å‚¨")
    print("=" * 60)
    
    print(f"ğŸ“ é…ç½®ä¿¡æ¯:")
    print(f"  åµŒå…¥æ¨¡å‹: {EMBEDDING_MODEL_NAME}")
    print(f"  å‘é‡ç»´åº¦: {EMBEDDING_DIM}")
    print(f"  é›†åˆåç§°: {COLLECTION_NAME}")
    print(f"  æ•°æ®æ–‡ä»¶: {DATA_FILE}")
    print()
    
    # 1. åŠ è½½æ•°æ®
    texts, metadata_list = load_and_prepare_data()
    if not texts:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥DATA_FILEé…ç½®")
        return
    
    # 2. åŠ è½½æ¨¡å‹
    print(f"ğŸ§  åŠ è½½åµŒå…¥æ¨¡å‹...")
    start_time = time.time()
    
    # å°è¯•ä½¿ç”¨ç¼“å­˜åŠ è½½
    model = load_embedding_model(EMBEDDING_MODEL_NAME)
    if not model:
        # ç›´æ¥åŠ è½½
        try:
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer(EMBEDDING_MODEL_NAME)
            print("âœ… ç›´æ¥åŠ è½½æ¨¡å‹æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("æç¤º: BAAI/bge-small-zh-v1.5 æ¨¡å‹çº¦400MBï¼Œé¦–æ¬¡ä¸‹è½½éœ€è¦æ—¶é—´")
            print("å¯ä»¥å°è¯•: pip install sentence-transformers")
            return
    
    model_load_time = time.time() - start_time
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ ({model_load_time:.1f}ç§’)")
    
    # 3. å‘é‡åŒ–
    embeddings = batch_vectorize(texts, model)
    if not embeddings:
        print("âŒ å‘é‡åŒ–å¤±è´¥")
        return
    
    # 4. å­˜å‚¨åˆ°Milvus
    success = store_in_milvus(embeddings, metadata_list)
    
    if success:
        print("\n" + "ğŸ‰" * 20)
        print("å‘é‡åŒ–ä¸å­˜å‚¨å®Œæˆï¼")
        print("ğŸ‰" * 20)
        print(f"\nğŸ“Š æ€»ç»“:")
        print(f"  æ–‡æ¡£æ•°é‡: {len(texts)}")
        print(f"  å‘é‡ç»´åº¦: {len(embeddings[0])}")
        print(f"  å­˜å‚¨æ–‡ä»¶: ./milvus_lite_data.db")
        print(f"  é›†åˆåç§°: {COLLECTION_NAME}")
        print(f"\nğŸš€ ä¸‹ä¸€æ­¥:")
        print("  è¿è¡Œ: streamlit run app.py")
        print("  è®¿é—®: http://localhost:8501")
    else:
        print("\nâŒ å‘é‡åŒ–ä¸å­˜å‚¨å¤±è´¥")

if __name__ == "__main__":
    main()