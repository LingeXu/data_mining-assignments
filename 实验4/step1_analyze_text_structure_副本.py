#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æmedical.jsonä¸­çš„æ–‡æœ¬ç»“æ„
"""

import json
import os
import re

def analyze_medical_text(file_path):
    """åˆ†æåŒ»ç–—æ–‡æœ¬çš„ç»“æ„"""
    print(f"ğŸ“‚ åˆ†ææ–‡ä»¶: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\nğŸ“Š JSONç»“æ„:")
    for key, value in data.items():
        print(f"  {key}: {type(value).__name__}")
    
    # è·å–contextæ–‡æœ¬
    context_text = data.get('context', '')
    if not context_text:
        print("âŒ contextå­—æ®µä¸ºç©º")
        return
    
    print(f"\nğŸ“ æ–‡æœ¬é•¿åº¦: {len(context_text)} å­—ç¬¦")
    print(f"ğŸ“„ æ–‡æœ¬å¤§å°: {len(context_text.encode('utf-8')) / 1024:.1f} KB")
    
    # åˆ†ææ–‡æœ¬ç»“æ„
    print(f"\nğŸ” æ–‡æœ¬ç»“æ„åˆ†æ:")
    
    # 1. æŸ¥çœ‹å¼€å¤´
    print("å¼€å¤´100å­—ç¬¦:")
    print("-" * 50)
    print(context_text[:100])
    print("-" * 50)
    
    # 2. æŸ¥çœ‹ä¸­é—´éƒ¨åˆ†
    if len(context_text) > 500:
        mid_start = len(context_text) // 2
        print(f"\nä¸­é—´éƒ¨åˆ† (ä½ç½®{mid_start}-{mid_start+100}):")
        print("-" * 50)
        print(context_text[mid_start:mid_start+100])
        print("-" * 50)
    
    # 3. æŸ¥çœ‹ç»“å°¾
    if len(context_text) > 200:
        print(f"\nç»“å°¾100å­—ç¬¦:")
        print("-" * 50)
        print(context_text[-100:])
        print("-" * 50)
    
    # 4. æŸ¥æ‰¾å¸¸è§çš„åˆ†éš”ç¬¦
    print(f"\nğŸ”§ æŸ¥æ‰¾æ–‡æœ¬åˆ†éš”æ¨¡å¼:")
    
    # æŸ¥æ‰¾å¯èƒ½çš„æ ‡é¢˜æˆ–åˆ†éš”ç¬¦
    patterns = [
        r'\n#+ ',  # Markdownæ ‡é¢˜
        r'\n\d+\.\s',  # æ•°å­—åˆ—è¡¨
        r'\nâ€¢\s',  # é¡¹ç›®ç¬¦å·
        r'\n-{3,}',  # åˆ†éš”çº¿
        r'\n[A-Z][a-z]+: ',  # æ ‡é¢˜æ ·å¼
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, context_text[:5000])  # åªæ£€æŸ¥å‰5000å­—ç¬¦
        if matches:
            print(f"  æ‰¾åˆ°æ¨¡å¼ '{pattern}': {len(matches)} æ¬¡")
            if matches:
                print(f"    ç¤ºä¾‹: {matches[0]}")
    
    # 5. æŒ‰æ®µè½åˆ†å‰²æŸ¥çœ‹
    paragraphs = [p for p in context_text.split('\n') if p.strip()]
    print(f"\nğŸ“‘ æ®µè½æ•°é‡ (æŒ‰æ¢è¡Œç¬¦): {len(paragraphs)}")
    if paragraphs:
        print(f"ç¬¬ä¸€æ®µ: {paragraphs[0][:150]}...")
        print(f"å¹³å‡æ®µè½é•¿åº¦: {sum(len(p) for p in paragraphs[:20])/len(paragraphs[:20]):.0f} å­—ç¬¦")
    
    # 6. æŸ¥æ‰¾å…³é”®è¯
    medical_keywords = ['ç™Œç—‡', 'æ²»ç–—', 'ç—‡çŠ¶', 'è¯Šæ–­', 'è¯ç‰©', 'åŒ»é™¢', 'åŒ»ç”Ÿ']
    print(f"\nğŸ¥ åŒ»ç–—å…³é”®è¯å‡ºç°æ¬¡æ•°:")
    for keyword in medical_keywords:
        count = context_text.count(keyword)
        if count > 0:
            print(f"  {keyword}: {count} æ¬¡")
    
    return context_text

if __name__ == "__main__":
    data_file = os.path.join(os.getcwd(), "data", "medical.json")
    
    if not os.path.exists(data_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        exit(1)
    
    print(f"ğŸ“ å·¥ä½œç›®å½•: {os.getcwd()}")
    print()
    
    text = analyze_medical_text(data_file)
    
    print(f"\nâœ… åˆ†æå®Œæˆ")
    print(f"\nğŸ’¡ å¯¹RAGç³»ç»Ÿçš„å¯ç¤º:")
    print("1. éœ€è¦å°†é•¿æ–‡æœ¬åˆ†å‰²æˆchunksï¼ˆåˆ†å—ï¼‰")
    print("2. åˆ†å—ç­–ç•¥å¾ˆé‡è¦ï¼šæŒ‰æ®µè½ã€å›ºå®šé•¿åº¦æˆ–è¯­ä¹‰åˆ†å‰²")
    print("3. é¢„å¤„ç†è„šæœ¬éœ€è¦å¤„ç†å­—ç¬¦ä¸²æ ¼å¼è€Œéåˆ—è¡¨æ ¼å¼")